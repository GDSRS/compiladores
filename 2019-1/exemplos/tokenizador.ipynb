{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizador.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "90bt5aGEC09y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import io\n",
        "import tokenize\n",
        "\n",
        "\n",
        "BLACKLIST = {tokenize.NL, tokenize.INDENT, tokenize.DEDENT, \n",
        "             tokenize.NEWLINE, tokenize.ENDMARKER, tokenize.ENCODING}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yPTLee5C099",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_lexer(defs, ignore=(), keywords=()):\n",
        "    \"\"\"\n",
        "    Retorna um lexer a partir de um dicionario com\n",
        "    definicoes de tokens\n",
        "    \"\"\"\n",
        "    defs['error'] = r'.+?'\n",
        "    named = [r'(?P<%s>%s)' % (k, v) \n",
        "             for (k, v) in defs.items()]\n",
        "    regex = re.compile('|'.join(named))\n",
        "    \n",
        "    def lexer(code):\n",
        "        tokens = []\n",
        "        line_no = 1\n",
        "        indent = 0\n",
        "        \n",
        "        for m in regex.finditer(code):\n",
        "            i, j = m.span()\n",
        "            data = m.string[i:j]\n",
        "            kind = m.lastgroup\n",
        "            \n",
        "            if kind == 'space':\n",
        "                line_no += data.count('\\n')\n",
        "            if data in keywords:\n",
        "                tokens.append(Token(data, 'keyword'))\n",
        "            elif kind == 'error':\n",
        "                raise ValueError(f'invalido: {data!r}')\n",
        "            elif kind not in ignore:\n",
        "                tokens.append((data, kind))\n",
        "            \n",
        "        return tokens\n",
        "        \n",
        "    return lexer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CRwU1QeC0-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "py_lex = lambda code: list(py_tokens(code))\n",
        "\n",
        "def py_tokens(code):\n",
        "    fd = io.BytesIO(code.encode('utf8'))\n",
        "    tks = tokenize.tokenize(fd.__next__)\n",
        "    next(tks)\n",
        "    for tk in tks:\n",
        "        if tk.type not in BLACKLIST:\n",
        "            yield (tk.string, tokenize.tok_name[tk.type])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDNdhdHaC0-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CRIE SEU LEXER AQUI\n",
        "my_lex = make_lexer({\n",
        "    'NAME': r'[a-zA-Z_]\\w*',\n",
        "    'SPACE': r'\\s+',\n",
        "    'ANY': r'.+?',\n",
        "}, ignore={'SPACE', 'COMMMENT'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHumBlA_C0-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d55f7056-5604-4f5a-abd5-430f06050b92"
      },
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "-42\n",
        "\"\"\"\n",
        "\n",
        "cmp = list(zip(my_lex(code), py_lex(code)))\n",
        "print('todos iguais?', all(x == y for x, y in cmp))\n",
        "cmp"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "todos iguais? False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('-', 'ANY'), ('-', 'OP')), (('4', 'ANY'), ('42', 'NUMBER'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "1hG0pDoVC0-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}